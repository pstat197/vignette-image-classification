{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cfa3386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp311-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Downloading torch-2.9.1-cp311-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m82.5 kB/s\u001b[0m  \u001b[33m0:11:01\u001b[0mm0:00:01\u001b[0m00:24\u001b[0mm\n",
      "\u001b[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading networkx-3.6-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m128.5 kB/s\u001b[0m  \u001b[33m0:00:16\u001b[0m eta \u001b[36m0:00:02\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached markupsafe-3.0.3-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [torch]32m7/8\u001b[0m [torch]]x]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 filelock-3.20.0 fsspec-2025.10.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.6 sympy-1.14.0 torch-2.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3668cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from vit_model import VisionTransformer\n",
    "from preprocessing import prepare_dl_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1e0ca65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (25000, 224, 224, 3), Labels: (25000,)\n",
      "Class distribution: [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data_dl/train'\n",
    "X, y = prepare_dl_data(data_path)\n",
    "print(f'Data shape: {X.shape}, Labels: {y.shape}')\n",
    "print(f'Class distribution: {np.bincount(y)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71221e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X).permute(0, 3, 1, 2)\n",
    "        self.y = torch.LongTensor(y)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "indices = np.random.permutation(len(X))\n",
    "split = int(0.8 * len(X))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_dataset = ImageDataset(X[train_idx], y[train_idx])\n",
    "val_dataset = ImageDataset(X[val_idx], y[val_idx])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e2661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on mps\n",
      "Total parameters: 11,019,650\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = VisionTransformer(img_size=224, patch_size=16, num_classes=2, \n",
    "                          embed_dim=384, depth=6, num_heads=6)\n",
    "model = model.to(device)\n",
    "print(f'Model on {device}')\n",
    "print(f'Total parameters: {sum(p.numel() for p in model.parameters()):,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c5328d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100/625\n",
      "  Batch 200/625\n",
      "  Batch 300/625\n",
      "  Batch 400/625\n",
      "  Batch 500/625\n",
      "  Batch 600/625\n",
      "Epoch 1/5 - Train Loss: 0.6963, Train Acc: 54.43%, Val Loss: 0.6600, Val Acc: 59.56%\n",
      "  Batch 100/625\n",
      "  Batch 200/625\n",
      "  Batch 300/625\n",
      "  Batch 400/625\n",
      "  Batch 500/625\n",
      "  Batch 600/625\n",
      "Epoch 2/5 - Train Loss: 0.6609, Train Acc: 59.97%, Val Loss: 0.6797, Val Acc: 57.36%\n",
      "  Batch 100/625\n",
      "  Batch 200/625\n",
      "  Batch 300/625\n",
      "  Batch 400/625\n",
      "  Batch 500/625\n",
      "  Batch 600/625\n",
      "Epoch 3/5 - Train Loss: 0.6393, Train Acc: 62.98%, Val Loss: 0.6359, Val Acc: 63.24%\n",
      "  Batch 100/625\n",
      "  Batch 200/625\n",
      "  Batch 300/625\n",
      "  Batch 400/625\n",
      "  Batch 500/625\n",
      "  Batch 600/625\n",
      "Epoch 4/5 - Train Loss: 0.6313, Train Acc: 63.62%, Val Loss: 0.6436, Val Acc: 63.66%\n",
      "  Batch 100/625\n",
      "  Batch 200/625\n",
      "  Batch 300/625\n",
      "  Batch 400/625\n",
      "  Batch 500/625\n",
      "  Batch 600/625\n",
      "Epoch 5/5 - Train Loss: 0.6118, Train Acc: 66.05%, Val Loss: 0.6099, Val Acc: 66.04%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.1)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    batch_count = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += predicted.eq(y_batch).sum().item()\n",
    "        batch_count += 1\n",
    "        if batch_count % 100 == 0:\n",
    "            print(f'  Batch {batch_count}/{len(train_loader)}')\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs} - Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "          f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss/len(val_loader):.4f}, '\n",
    "          f'Val Acc: {val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41cbfaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions:\n",
      "True: cat, Pred: cat, Confidence: 65.62%\n",
      "True: dog, Pred: cat, Confidence: 63.68%\n",
      "True: cat, Pred: cat, Confidence: 52.38%\n",
      "True: cat, Pred: cat, Confidence: 78.83%\n",
      "True: dog, Pred: dog, Confidence: 54.71%\n",
      "True: dog, Pred: dog, Confidence: 52.71%\n",
      "True: dog, Pred: dog, Confidence: 57.68%\n",
      "True: dog, Pred: cat, Confidence: 50.52%\n",
      "True: dog, Pred: cat, Confidence: 59.05%\n",
      "True: cat, Pred: dog, Confidence: 50.57%\n",
      "True: dog, Pred: dog, Confidence: 70.01%\n",
      "True: dog, Pred: dog, Confidence: 50.31%\n",
      "True: cat, Pred: dog, Confidence: 50.22%\n",
      "True: dog, Pred: cat, Confidence: 51.28%\n",
      "True: dog, Pred: dog, Confidence: 53.89%\n",
      "True: cat, Pred: cat, Confidence: 54.29%\n",
      "True: dog, Pred: cat, Confidence: 60.17%\n",
      "True: dog, Pred: dog, Confidence: 88.49%\n",
      "True: dog, Pred: cat, Confidence: 67.38%\n",
      "True: dog, Pred: dog, Confidence: 54.10%\n",
      "True: cat, Pred: cat, Confidence: 68.28%\n",
      "True: dog, Pred: cat, Confidence: 72.56%\n",
      "True: dog, Pred: cat, Confidence: 61.57%\n",
      "True: dog, Pred: dog, Confidence: 56.56%\n",
      "True: cat, Pred: cat, Confidence: 51.58%\n",
      "True: dog, Pred: cat, Confidence: 59.79%\n",
      "True: cat, Pred: cat, Confidence: 69.37%\n",
      "True: dog, Pred: dog, Confidence: 91.26%\n",
      "True: dog, Pred: dog, Confidence: 91.23%\n",
      "True: cat, Pred: dog, Confidence: 59.37%\n",
      "True: cat, Pred: cat, Confidence: 65.93%\n",
      "True: dog, Pred: dog, Confidence: 70.51%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test, y_test = next(iter(val_loader))\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = outputs.max(1)\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    \n",
    "    print('Sample predictions:')\n",
    "    for i in range(len(y_test)):\n",
    "        true_label = 'cat' if y_test[i] == 0 else 'dog'\n",
    "        pred_label = 'cat' if predicted[i] == 0 else 'dog'\n",
    "        confidence = probs[i][predicted[i]].item() * 100\n",
    "        print(f'True: {true_label}, Pred: {pred_label}, Confidence: {confidence:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2757d37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (1.0.22)\n",
      "Requirement already satisfied: torch in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from timm) (2.9.1)\n",
      "Requirement already satisfied: torchvision in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from timm) (0.24.1)\n",
      "Requirement already satisfied: pyyaml in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from timm) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from timm) (1.1.7)\n",
      "Requirement already satisfied: safetensors in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from timm) (0.7.0)\n",
      "Requirement already satisfied: filelock in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from huggingface_hub->timm) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from huggingface_hub->timm) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from huggingface_hub->timm) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from huggingface_hub->timm) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from huggingface_hub->timm) (25.0)\n",
      "Requirement already satisfied: shellingham in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from huggingface_hub->timm) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from huggingface_hub->timm) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Requirement already satisfied: anyio in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.12.0)\n",
      "Requirement already satisfied: certifi in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from torch->timm) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from jinja2->torch->timm) (3.0.3)\n",
      "Requirement already satisfied: numpy in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from torchvision->timm) (2.3.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from torchvision->timm) (12.0.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/madhav/vignette-image-classification/.venv/lib/python3.11/site-packages (from typer-slim->huggingface_hub->timm) (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model on mps\n",
      "Total parameters: 85,800,194\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "pretrained_model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=2)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "print(f'Pretrained model on {device}')\n",
    "print(f'Total parameters: {sum(p.numel() for p in pretrained_model.parameters()):,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5524e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 100/625\n",
      "  Batch 200/625\n",
      "  Batch 300/625\n",
      "  Batch 400/625\n",
      "  Batch 500/625\n",
      "  Batch 600/625\n",
      "PRETRAINED Epoch 1/3 - Train Loss: 0.0885, Train Acc: 96.62%, Val Loss: 0.0635, Val Acc: 97.56%\n",
      "  Batch 100/625\n",
      "  Batch 200/625\n",
      "  Batch 300/625\n",
      "  Batch 400/625\n",
      "  Batch 500/625\n",
      "  Batch 600/625\n",
      "PRETRAINED Epoch 2/3 - Train Loss: 0.0628, Train Acc: 97.55%, Val Loss: 0.0697, Val Acc: 97.62%\n",
      "  Batch 100/625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m outputs = pretrained_model(X_batch)\n\u001b[32m     16\u001b[39m loss = criterion_pretrained(outputs, y_batch)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer_pretrained.step()\n\u001b[32m     20\u001b[39m train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vignette-image-classification/.venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vignette-image-classification/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vignette-image-classification/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "criterion_pretrained = nn.CrossEntropyLoss()\n",
    "optimizer_pretrained = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "epochs_pretrained = 3\n",
    "for epoch in range(epochs_pretrained):\n",
    "    pretrained_model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    batch_count = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer_pretrained.zero_grad()\n",
    "        outputs = pretrained_model(X_batch)\n",
    "        loss = criterion_pretrained(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer_pretrained.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += y_batch.size(0)\n",
    "        train_correct += predicted.eq(y_batch).sum().item()\n",
    "        batch_count += 1\n",
    "        if batch_count % 100 == 0:\n",
    "            print(f'  Batch {batch_count}/{len(train_loader)}')\n",
    "    \n",
    "    pretrained_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = pretrained_model(X_batch)\n",
    "            loss = criterion_pretrained(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += y_batch.size(0)\n",
    "            val_correct += predicted.eq(y_batch).sum().item()\n",
    "    \n",
    "    train_acc = 100. * train_correct / train_total\n",
    "    val_acc = 100. * val_correct / val_total\n",
    "    \n",
    "    print(f'PRETRAINED Epoch {epoch+1}/{epochs_pretrained} - Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "          f'Train Acc: {train_acc:.2f}%, Val Loss: {val_loss/len(val_loader):.4f}, '\n",
    "          f'Val Acc: {val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04de002a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e45fc9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.62"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fbabeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
